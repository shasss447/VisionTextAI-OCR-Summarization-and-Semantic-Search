{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "# import easyocr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TesseractProcessor:\n",
    "    def __init__(self, output_dir: str = \"tes_extracted_content\"):\n",
    "        \"\"\"\n",
    "        Initialize the document processor for images.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Directory to save extracted images\n",
    "        \"\"\"\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        # Set Tesseract path - modify this according to your installation\n",
    "        pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "        # initializing model and tokenizer\n",
    "        self.model_name = \"facebook/bart-large-cnn\" \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        # Load SpaCy model for NLP tasks\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except OSError:\n",
    "            spacy.cli.download('en_core_web_sm')\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = output_dir\n",
    "        self.images_dir = os.path.join(output_dir, \"images\")\n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        \n",
    "    def process_document(self, image_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a document image to extract text and images.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the document image\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Extracted content including text, entities, and image paths\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            # Read and process image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not read image at {image_path}\")\n",
    "            \n",
    "            # Extract text content\n",
    "            text_result = self.extract_text(image)\n",
    "            \n",
    "            # Extract images\n",
    "            image_result = self.extract_images(image, Path(image_path).stem)\n",
    "            processing_time = time.time() - start_time\n",
    "            return {\n",
    "                **text_result,\n",
    "                'extracted_images': image_result,\n",
    "                'processing_time': processing_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing document: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_text(self, image: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract and process text from image.\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Input image\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Processed text data\n",
    "        \"\"\"\n",
    "        # Preprocess image for better OCR\n",
    "        processed_image = self.preprocess_for_ocr(image)\n",
    "        \n",
    "        # Perform OCR\n",
    "        raw_text = pytesseract.image_to_string(processed_image)\n",
    "        \n",
    "        # Get OCR confidence data\n",
    "        ocr_data = pytesseract.image_to_data(processed_image, output_type=pytesseract.Output.DICT)\n",
    "        \n",
    "        # Process extracted text\n",
    "        processed_data = self.process_text(raw_text)\n",
    "        summary=self.summary_generator(processed_data['processed_text'])\n",
    "        \n",
    "        return {\n",
    "            'raw_text': raw_text,\n",
    "            'processed_text': processed_data['processed_text'],\n",
    "            'summary':summary,\n",
    "            'entities': processed_data['entities'],\n",
    "            'pos_tags': processed_data['pos_tags'],\n",
    "            'confidence_scores': self._get_confidence_scores(ocr_data)\n",
    "        }\n",
    "\n",
    "    def extract_images(self, image: np.ndarray, base_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract images from document using contour detection.\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Input image\n",
    "            base_name (str): Base name for saving extracted images\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of extracted image information\n",
    "        \"\"\"\n",
    "        extracted = []\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply different thresholding methods to detect various types of images\n",
    "        binary_methods = [\n",
    "            ('otsu', cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]),\n",
    "            # ('adaptive', cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "            #                                 cv2.THRESH_BINARY_INV, 11, 2))\n",
    "        ]\n",
    "        \n",
    "        for method_name, binary in binary_methods:\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            for idx, contour in enumerate(contours):\n",
    "                # Filter small contours and check aspect ratio\n",
    "                if not self._is_valid_image_region(contour, image.shape):\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                \n",
    "                # Extract region\n",
    "                roi = image[y:y+h, x:x+w]\n",
    "                \n",
    "                # Check if region contains enough variation to be an image\n",
    "                if not self._is_image_content(roi):\n",
    "                    continue\n",
    "                \n",
    "                # Save extracted region\n",
    "                image_filename = f\"{base_name}_region_{idx + 1}.png\"\n",
    "                image_path = os.path.join(self.images_dir, image_filename)\n",
    "                \n",
    "                cv2.imwrite(image_path, roi)\n",
    "                \n",
    "                extracted.append({\n",
    "                    'filename': image_filename,\n",
    "                    'path': image_path,\n",
    "                    'position': {'x': x, 'y': y, 'width': w, 'height': h},\n",
    "                    'area': cv2.contourArea(contour),\n",
    "                })\n",
    "        \n",
    "        return extracted\n",
    "\n",
    "    def _is_valid_image_region(self, contour: np.ndarray, image_shape: Tuple) -> bool:\n",
    "        \"\"\"\n",
    "        Check if contour is likely to be an image region.\n",
    "        \"\"\"\n",
    "        # Get area and dimensions\n",
    "        area = cv2.contourArea(contour)\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = w / h if h != 0 else 0\n",
    "        \n",
    "        # Calculate relative size\n",
    "        image_area = image_shape[0] * image_shape[1]\n",
    "        relative_size = area / image_area\n",
    "        \n",
    "        # Define thresholds\n",
    "        MIN_RELATIVE_SIZE = 0.01  # 1% of image\n",
    "        MAX_RELATIVE_SIZE = 0.9   # 90% of image\n",
    "        MIN_ASPECT_RATIO = 0.2\n",
    "        MAX_ASPECT_RATIO = 5.0\n",
    "        \n",
    "        return (MIN_RELATIVE_SIZE <= relative_size <= MAX_RELATIVE_SIZE and \n",
    "                MIN_ASPECT_RATIO <= aspect_ratio <= MAX_ASPECT_RATIO)\n",
    "\n",
    "    def _is_image_content(self, region: np.ndarray, std_threshold: float = 20) -> bool:\n",
    "        \"\"\"\n",
    "        Check if region contains enough variation to be an image.\n",
    "        \"\"\"\n",
    "        gray_region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "        return np.std(gray_region) > std_threshold\n",
    "\n",
    "    def preprocess_for_ocr(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess image for better OCR results.\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply thresholding\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Denoise\n",
    "        denoised = cv2.fastNlMeansDenoising(binary)\n",
    "        \n",
    "        # Deskew if needed\n",
    "        angle = self._get_skew_angle(denoised)\n",
    "        if abs(angle) > 0.5:\n",
    "            denoised = self._rotate_image(denoised, angle)\n",
    "        \n",
    "        return denoised\n",
    "\n",
    "    def _get_skew_angle(self, image: np.ndarray) -> float:\n",
    "        \"\"\"Calculate skew angle of the image.\"\"\"\n",
    "        coords = np.column_stack(np.where(image > 0))\n",
    "        angle = cv2.minAreaRect(coords)[-1]\n",
    "        if angle < -45:\n",
    "            angle = 90 + angle\n",
    "        return -angle\n",
    "\n",
    "    def _rotate_image(self, image: np.ndarray, angle: float) -> np.ndarray:\n",
    "        \"\"\"Rotate the image by given angle.\"\"\"\n",
    "        (h, w) = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        return rotated\n",
    "\n",
    "    def process_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process extracted text using NLP techniques.\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        \n",
    "        # Process with SpaCy\n",
    "        doc = self.nlp(cleaned_text)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Get POS tags\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        \n",
    "        return {\n",
    "            'processed_text': cleaned_text,\n",
    "            'entities': entities,\n",
    "            'pos_tags': pos_tags\n",
    "        }\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean the extracted text.\"\"\"\n",
    "        # Remove special characters and extra whitespace\n",
    "        cleaned = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        \n",
    "        # Remove common OCR artifacts\n",
    "        cleaned = re.sub(r'[|]', 'I', cleaned)\n",
    "        cleaned = re.sub(r'[Â¢]', 'c', cleaned)\n",
    "        \n",
    "        return cleaned.strip()\n",
    "\n",
    "    def summary_generator(self, text:str)->str:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=200,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def _get_confidence_scores(self, ocr_data: Dict[str, Any]) -> List[float]:\n",
    "        \"\"\"Extract confidence scores for OCR results.\"\"\"\n",
    "        return [conf for conf in ocr_data['conf'] if conf != -1]\n",
    "\n",
    "\n",
    "Tprocessor = TesseractProcessor()\n",
    "    # Eprocessor = EasyOCRProcessor()\n",
    "\n",
    "        # Process a document\n",
    "Tresult1 = [Tprocessor.process_document(\"dataset/News/94682942.jpg\")]\n",
    "\n",
    "    #     Eresult = Eprocessor.process_document(\"dataset/News/94682942.jpg\")\n",
    "    #     # Print performance metrics\n",
    "    #     print(\"\\nPerformance Metrics:\")\n",
    "    #     print(f\"Processing Time: {Tresult['processing_time']:.2f} seconds\")\n",
    "    #     print(f\"Processing Time: {Eresult['processing_time']:.2f} seconds\")\n",
    "    #     print(f\"Confidence Score: {np.mean(Tresult['confidence_scores']):.2f},{np.mean(Eresult['confidence_scores']):.2f}\")\n",
    "        \n",
    "    #     # Print text analysis\n",
    "    #     print(\"\\nExtracted Text Sample:\")\n",
    "    #     print(Tresult['processed_text'][:200] + \"...\")\n",
    "    #     print(Eresult['processed_text'][:200] + \"...\")\n",
    "        \n",
    "    #     print(\"\\nNamed Entities:\")\n",
    "    #     for entity, label in Tresult['entities']:\n",
    "    #         print(f\"{entity}: {label}\")\n",
    "    #     print(\"--------------\")\n",
    "    #     for entity, label, _ in Eresult['entities'][:5]:\n",
    "    #         print(f\"{entity}: {label}\")\n",
    "        \n",
    "    #     print(\"\\nPOS Tags Sample:\")\n",
    "    #     for token, pos in Tresult['pos_tags'][:10]:\n",
    "    #         print(f\"{token}: {pos}\")\n",
    "    #     print(\"--------------------\")\n",
    "    #     for token, pos, tag, dep in Eresult['pos_tags'][:5]:\n",
    "    #         print(f\"{token}: {pos} ({tag}) - {dep}\")\n",
    "\n",
    "    #     print(\"\\nExtracted Images:\")\n",
    "    #     for img in Tresult['extracted_images']:\n",
    "    #         print(f\"Method: {img['method']}\")\n",
    "    #         print(f\"Size: {img['size']}\")\n",
    "    #         print(f\"Saved to: {img['filename']}\")\n",
    "    #     print(\"----------------\")\n",
    "    #     for img in Eresult['extracted_images']:\n",
    "    #         print(f\"Saved: {img['filename']}\")\n",
    "    #         print(f\"Method: {img['method']}\")\n",
    "    #         print(f\"Position: {img['position']}\")\n",
    "    #         print(\"---\")\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # summarizer = pipeline(\"summarization\", model=\"ainize/bart-base-cnn\")\n",
    "    # print(Tresult['processed_text'])\n",
    "    # print(summarizer(Tresult['processed_text'], max_length=230, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pymilvus import (MilvusClient,DataType,AnnSearchRequest,RRFRanker,Collection)\n",
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "import torch\n",
    "import timm\n",
    "from sklearn.preprocessing import normalize\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "\n",
    "class PostGreDatabaseHandler:\n",
    "    def __init__(self, postgres_conn_string: str):\n",
    "        \"\"\"\n",
    "        Initialize database connections and models.\n",
    "        \n",
    "        Args:\n",
    "            postgres_conn_string: PostgreSQL connection string\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize PostgreSQL connection\n",
    "        self.pg_conn = psycopg2.connect(postgres_conn_string)\n",
    "\n",
    "        \"\"\"\n",
    "        Create the necessary tables in PostgreSQL database.\n",
    "    \n",
    "        Args:\n",
    "        conn_params (dict): Database connection parameters\n",
    "        \"\"\"\n",
    "        commands = (\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "           id SERIAL PRIMARY KEY,\n",
    "           file_name VARCHAR(255) NOT NULL,\n",
    "           content TEXT NOT NULL,\n",
    "           summary TEXT NOT NULL,\n",
    "           confidence FLOAT NOT NULL\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS named_entities (\n",
    "           id SERIAL PRIMARY KEY,\n",
    "           doc_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,\n",
    "           entity TEXT NOT NULL,\n",
    "           label VARCHAR(50) NOT NULL\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS pos_tags (\n",
    "           id SERIAL PRIMARY KEY,\n",
    "           doc_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,\n",
    "           token TEXT NOT NULL,\n",
    "           pos_tag VARCHAR(20) NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "        )\n",
    "    \n",
    "        try:\n",
    "            with self.pg_conn.cursor() as cur:\n",
    "\n",
    "              # Create each table\n",
    "              for command in commands:\n",
    "                cur.execute(command)\n",
    "            \n",
    "              cur.close()\n",
    "              self.pg_conn.commit()\n",
    "        \n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            print(f\"Error: {error}\")\n",
    "\n",
    "\n",
    "    def store_document_data(self, doc_data: List[Dict[str, Any]]):\n",
    "      \"\"\"\n",
    "      Store document data including file name, text, entities, and POS tags into PostgreSQL tables.\n",
    "    \n",
    "      Args:\n",
    "        doc_data (List[Dict[str, Any]]): Dictionary containing:\n",
    "            - file_name (str): Name of the file\n",
    "            - text (str): Content of the file\n",
    "            - summary (str): Summary of the content of the file\n",
    "            - confidence_score (float): Confidence score for the document\n",
    "            - entities (Dict[str, str]): Dictionary of entity:label pairs\n",
    "            - pos (Dict[str, str]): Dictionary of text:pos_tag pairs\n",
    "            - postgres_conn_string: PostgreSQL connection string\n",
    "      \"\"\"\n",
    "      try:\n",
    "        with self.pg_conn.cursor() as cur:\n",
    "            for data in doc_data:\n",
    "                # First, insert the document and get its ID\n",
    "                doc_insert_query = \"\"\"\n",
    "                  INSERT INTO documents (file_name, content,summary, confidence)\n",
    "                  VALUES (%s, %s, %s, %s)\n",
    "                  RETURNING id;\n",
    "                \"\"\"\n",
    "                cur.execute(doc_insert_query, (data['file_name'],data['processed_text'],data['summary'],data['confidence_scores']))\n",
    "                doc_id = cur.fetchone()[0]\n",
    "            \n",
    "                # Insert named entities\n",
    "                if data.get('entities'):\n",
    "                  entities_insert_query = \"\"\"\n",
    "                    INSERT INTO named_entities (doc_id, entity, label)\n",
    "                    VALUES (%s, %s, %s);\n",
    "                \"\"\"\n",
    "                  entity_data = [\n",
    "                    (doc_id, entity, label)for entity, label in data['entities']\n",
    "                    ]\n",
    "                cur.executemany(entities_insert_query, entity_data)\n",
    "            \n",
    "                # Insert POS tags\n",
    "                if data.get('pos_tags'):\n",
    "                  pos_insert_query = \"\"\"\n",
    "                    INSERT INTO pos_tags (doc_id, token, pos_tag)\n",
    "                    VALUES (%s, %s, %s);\n",
    "                \"\"\"\n",
    "                  pos_data = [\n",
    "                    (doc_id, token, pos_tag)for token, pos_tag in data['pos_tags']\n",
    "                    ]\n",
    "                cur.executemany(pos_insert_query, pos_data)\n",
    "            \n",
    "            self.pg_conn.commit()\n",
    "            \n",
    "      except Exception as e:\n",
    "        self.pg_conn.rollback()\n",
    "        raise Exception(f\"Error storing document data: {str(e)}\")\n",
    "      \n",
    "    def postgres_file_entities(self, file_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Search for a specific file name and return entities from the documents table.\n",
    "\n",
    "        Args:\n",
    "         file_name (str): Exact file name to search for\n",
    "\n",
    "        Returns:\n",
    "         dict: including entities and label.\n",
    "              Returns empty dict if file not found.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "          SELECT entity,label\n",
    "          FROM named_entities\n",
    "          JOIN documents ON named_entities.doc_id = documents.id\n",
    "          WHERE documents.file_name = %s;\n",
    "        \"\"\"\n",
    "    \n",
    "        try:\n",
    "          cur = self.pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "          cur.execute(query, (file_name,))\n",
    "          result = cur.fetchall()\n",
    "          self.pg_conn.commit()\n",
    "          return result if result else {}\n",
    "          \n",
    "        except Exception as e:\n",
    "          self.logger.error(f\"Error in file search: {str(e)}\")\n",
    "          if self.pg_conn:\n",
    "            self.pg_conn.rollback()  # Rollback the transaction on error\n",
    "        return {}\n",
    "    \n",
    "    def postgres_entities_file(self, entity: str) -> dict:\n",
    "        \"\"\"\n",
    "        Search for entities and return file names.\n",
    "\n",
    "        Args:\n",
    "         entity (str): entity to search for\n",
    "\n",
    "        Returns:\n",
    "         dict: including file names.\n",
    "              Returns empty dict if file not found.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "          SELECT distinct file_name\n",
    "          FROM documents\n",
    "          JOIN named_entities ON documents.id=named_entities.doc_id\n",
    "          WHERE named_entities.label = %s;\n",
    "        \"\"\"\n",
    "    \n",
    "        try:\n",
    "          cur = self.pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "          cur.execute(query, (entity,))\n",
    "          result = cur.fetchall()\n",
    "          self.pg_conn.commit()\n",
    "          return result if result else {}\n",
    "        \n",
    "        except Exception as e:\n",
    "          self.logger.error(f\"Error in file search: {str(e)}\")\n",
    "          if self.pg_conn:\n",
    "            self.pg_conn.rollback()  # Rollback the transaction on error\n",
    "        return {}\n",
    "      \n",
    "    def postgres_file_summary(self, file_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Search for a specific file name and return its details from the documents table.\n",
    "\n",
    "        Args:\n",
    "         file_name (str): Exact file name to search for\n",
    "\n",
    "        Returns:\n",
    "         dict: Document details including file name, content, and confidence score.\n",
    "              Returns empty dict if file not found.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "          SELECT\n",
    "          summary\n",
    "          FROM documents\n",
    "          WHERE file_name = %s\n",
    "          LIMIT 1;\n",
    "        \"\"\"\n",
    "    \n",
    "        try:\n",
    "          cur = self.pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "          cur.execute(query, (file_name,))\n",
    "          result = cur.fetchone()\n",
    "          self.pg_conn.commit()  # Commit the transaction\n",
    "          return result if result else {}\n",
    "        \n",
    "        except Exception as e:\n",
    "          self.logger.error(f\"Error in file search: {str(e)}\")\n",
    "          if self.pg_conn:\n",
    "            self.pg_conn.rollback()  # Rollback the transaction on error\n",
    "        return {}\n",
    "    \n",
    "    def postgrekeyword_search(self, keyword: str) -> List[dict]:\n",
    "       \"\"\"\n",
    "       Search for documents containing the given keyword and return their details.\n",
    "    \n",
    "       Args:\n",
    "        keyword (str): Keyword to search for in documents content and file names\n",
    "    \n",
    "      Returns:\n",
    "        List[dict]: List of matching documents with their file names and content\n",
    "       \"\"\"\n",
    "       query = \"\"\"\n",
    "        SELECT DISTINCT \n",
    "        file_name, content\n",
    "        FROM documents\n",
    "        WHERE content ILIKE %s OR file_name ILIKE %s\n",
    "        ORDER BY file_name;\n",
    "        \"\"\"\n",
    "       try:\n",
    "         cur = self.pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "         # Add wildcards for partial matching\n",
    "         search_pattern = f'%{keyword}%'\n",
    "         cur.execute(query, (search_pattern, search_pattern))\n",
    "         results = cur.fetchall()\n",
    "         return results\n",
    "            \n",
    "       except Exception as e:\n",
    "         self.logger.error(f\"Error in keyword search: {str(e)}\")\n",
    "       if self.pg_conn:\n",
    "         self.pg_conn.rollback()  # Rollback the transaction on error\n",
    "       return []\n",
    "\n",
    "    def close(self):\n",
    "      \"\"\"Close database connections.\"\"\"\n",
    "      if hasattr(self, 'pg_conn') and not self.pg_conn.closed:\n",
    "            self.pg_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgdb=PostGreDatabaseHandler(\"postgresql://postgres:12072024@localhost:5432/postgres\")\n",
    "pgdb.store_document_data(Tresult1)\n",
    "pgdb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, modelname):\n",
    "        # Load the pre-trained model\n",
    "        self.model = timm.create_model(\n",
    "            modelname, pretrained=True, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        # Get the input size required by the model\n",
    "        self.input_size = self.model.default_cfg[\"input_size\"]\n",
    "\n",
    "        config = resolve_data_config({}, model=modelname)\n",
    "        # Get the preprocessing function provided by TIMM for the model\n",
    "        self.preprocess = create_transform(**config)\n",
    "\n",
    "    def __call__(self, imagepath):\n",
    "        # Preprocess the input image\n",
    "        input_image = Image.open(imagepath).convert(\"RGB\")  # Convert to RGB if needed\n",
    "        input_image = self.preprocess(input_image)\n",
    "\n",
    "        # Convert the image to a PyTorch tensor and add a batch dimension\n",
    "        input_tensor = input_image.unsqueeze(0)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        # Extract the feature vector\n",
    "        feature_vector = output.squeeze().numpy()\n",
    "\n",
    "        return normalize(feature_vector.reshape(1, -1), norm=\"l2\").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusDatabaseHandler:\n",
    "    def __init__(self,milvusdb:str):\n",
    "        \"\"\"\n",
    "        Initialize database connections and models.\n",
    "\n",
    "        Args:\n",
    "            postgres_conn_string: PostgreSQL connection string\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialize Milvus Client\n",
    "        self.client=MilvusClient(milvusdb)\n",
    "\n",
    "        # Embedding Model\n",
    "        self.bge=BGEM3EmbeddingFunction(\n",
    "        model_name='BAAI/bge-m3',\n",
    "        device='cpu',\n",
    "        use_fp16=False\n",
    "        )\n",
    "\n",
    "        # Image Embedding\n",
    "        self.extractor = FeatureExtractor(\"resnet34\")\n",
    "\n",
    "        # Creating schema for texts\n",
    "        text_schema=MilvusClient.create_schema(\n",
    "            enable_dynamic_filter=True,\n",
    "            )\n",
    "\n",
    "        # Addings fields\n",
    "        text_schema.add_field(field_name=\"file_name\",datatype=DataType.VARCHAR,is_primary=True,max_length=100)\n",
    "        text_schema.add_field(field_name=\"Text\",datatype=DataType.VARCHAR,max_length=1000)\n",
    "        text_schema.add_field(field_name=\"Summary\",datatype=DataType.VARCHAR,max_length=100)\n",
    "        text_schema.add_field(field_name=\"sparse\",datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "        text_schema.add_field(field_name=\"dense\",datatype=DataType.FLOAT_VECTOR,dim=self.bge.dim[\"dense\"])\n",
    "        text_schema.add_field(field_name=\"confidence\",datatype=DataType.FLOAT)\n",
    "\n",
    "        # Creating Index\n",
    "        text_index_param=self.client.prepare_index_params()\n",
    "        text_index_param.add_index(\n",
    "            field_name=\"dense\",\n",
    "            index_name=\"dense_index\",\n",
    "            index_type=\"AUTOINDEX\",\n",
    "            metric_type=\"IP\",\n",
    "            params={\"nlist\":128}\n",
    "            )\n",
    "\n",
    "        text_index_param.add_index(\n",
    "            field_name=\"sparse\",\n",
    "            index_name=\"sparse_index\",\n",
    "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "            metric_type=\"IP\",\n",
    "            params={\"drop_ratio_build\":0.2},\n",
    "            )\n",
    "\n",
    "        # Creating Collection\n",
    "        if self.client.has_collection(collection_name=\"Text_collection\"):\n",
    "            self.client.drop_collection(collection_name=\"Text_collection\")\n",
    "        self.client.create_collection(\n",
    "             collection_name=\"Text_collection\",\n",
    "             schema=text_schema,\n",
    "             index_params=text_index_param\n",
    "            )\n",
    "\n",
    "        # Creating schema for Named-Entities\n",
    "        named_schema = MilvusClient.create_schema(\n",
    "            auto_id=True,\n",
    "            enable_dynamic_filter=True,\n",
    "            )\n",
    "\n",
    "        # Adding fields\n",
    "        named_schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "        named_schema.add_field(field_name=\"file_name\", datatype=DataType.VARCHAR,max_length=1000)\n",
    "        named_schema.add_field(field_name=\"entity\", datatype=DataType.VARCHAR, max_length=200)\n",
    "        named_schema.add_field(field_name=\"label\", datatype=DataType.VARCHAR, max_length=50)\n",
    "\n",
    "        # Creating Collection\n",
    "        if self.client.has_collection(collection_name=\"Named_collection\"):\n",
    "            self.client.drop_collection(collection_name=\"Named_collection\")\n",
    "        self.client.create_collection(\n",
    "             collection_name=\"Named_collection\",\n",
    "             schema=named_schema,\n",
    "            )\n",
    "\n",
    "        # Creating schema for Pos\n",
    "        pos_schema = MilvusClient.create_schema(\n",
    "            auto_id=True,\n",
    "            enable_dynamic_field=True,\n",
    "            )\n",
    "\n",
    "        # Adding fields\n",
    "        pos_schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "        pos_schema.add_field(field_name=\"file_name\", datatype=DataType.VARCHAR,max_length=1000)\n",
    "        pos_schema.add_field(field_name=\"token\", datatype=DataType.VARCHAR, max_length=100)\n",
    "        pos_schema.add_field(field_name=\"pos\", datatype=DataType.VARCHAR, max_length=20)\n",
    "\n",
    "        # Creating Collection\n",
    "        if self.client.has_collection(collection_name=\"Pos_collection\"):\n",
    "            self.client.drop_collection(collection_name=\"Pos_collection\")\n",
    "        self.client.create_collection(\n",
    "             collection_name=\"Pos_collection\",\n",
    "             schema=pos_schema,\n",
    "            )\n",
    "\n",
    "        # Creating schema for image\n",
    "        img_schema = MilvusClient.create_schema(\n",
    "            auto_id=True,\n",
    "            enable_dynamic_field=True,\n",
    "            )\n",
    "\n",
    "        # Adding fields\n",
    "        img_schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "        img_schema.add_field(field_name=\"file_name\", datatype=DataType.VARCHAR,max_length=1000)\n",
    "        img_schema.add_field(field_name=\"img_file_name\", datatype=DataType.VARCHAR, max_length=100)\n",
    "        img_schema.add_field(field_name=\"img_file_path\", datatype=DataType.VARCHAR, max_length=100)\n",
    "        img_schema.add_field(field_name=\"dense\",datatype=DataType.FLOAT_VECTOR,dim=512)\n",
    "\n",
    "        # Creating Index\n",
    "        img_index_param=self.client.prepare_index_params()\n",
    "        img_index_param.add_index(\n",
    "            field_name=\"dense\",\n",
    "            index_name=\"dense_index\",\n",
    "            index_type=\"AUTOINDEX\",\n",
    "            metric_type=\"COSINE\",\n",
    "            params={\"nlist\":128}\n",
    "            )\n",
    "\n",
    "        # Creating collection\n",
    "        if self.client.has_collection(collection_name=\"Img_collection\"):\n",
    "            self.client.drop_collection(collection_name=\"Img_collection\")\n",
    "        self.client.create_collection(\n",
    "             collection_name=\"Img_collection\",\n",
    "             schema=img_schema,\n",
    "             index_params=img_index_param\n",
    "            )\n",
    "\n",
    "\n",
    "    def store_document_data(self,doc_data:List[Dict[str, Any]]):\n",
    "      \"\"\"\n",
    "      Store document data including file name, text, entities, and POS tags into Milvus Collections.\n",
    "\n",
    "      Args:\n",
    "        doc_data (List[Dict[str, Any]]): Dictionary containing:\n",
    "            - file_name (str): Name of the file\n",
    "            - text (str): Content of the file\n",
    "            - summary (str): Summary of the content of the file\n",
    "            - confidence_score (float): Confidence score for the document\n",
    "            - entities (Dict[str, str]): Dictionary of entity:label pairs\n",
    "            - pos_tags (Dict[str, str]): Dictionary of text:pos_tag pairs\n",
    "            - images (list(Dict[str,str])): List of dictionary of images\n",
    "      \"\"\"\n",
    "\n",
    "      try:\n",
    "        for data in doc_data:\n",
    "            # Generate text embeddings\n",
    "            embeddings = self.bge.encode_documents([data['processed_text']])\n",
    "            sparse_vector = embeddings['sparse']\n",
    "            dense_vector = embeddings['dense'][0]\n",
    "            # Insert into Text_collection\n",
    "            text_data = [{\n",
    "                'file_name': data['file_name'],\n",
    "                'Text': data['processed_text'],\n",
    "                'summary':data['summary'],\n",
    "                'sparse': sparse_vector,\n",
    "                'dense': dense_vector,\n",
    "                'confidence': data['confidence_scores']\n",
    "            }]\n",
    "\n",
    "            text_insert_result = self.client.insert(\n",
    "                collection_name=\"Text_collection\",\n",
    "                data=text_data\n",
    "            )\n",
    "\n",
    "            # Insert into Named_collection\n",
    "            if data.get('entities'):\n",
    "                named_data = [\n",
    "                    {\n",
    "                        'file_name': data['file_name'],\n",
    "                        'entity': entity,\n",
    "                        'label': label\n",
    "                    }\n",
    "                    for entity, label in data['entities']\n",
    "                ]\n",
    "                self.client.insert(\n",
    "                    collection_name=\"Named_collection\",\n",
    "                    data=named_data\n",
    "                )\n",
    "\n",
    "            # Insert into Pos_collection\n",
    "            if data.get('pos_tags'):\n",
    "                pos_data = [\n",
    "                    {\n",
    "                        'file_name': data['file_name'],\n",
    "                        'token': token,\n",
    "                        'pos': pos\n",
    "                    }\n",
    "                    for token, pos in data['pos_tags']\n",
    "                ]\n",
    "                self.client.insert(\n",
    "                    collection_name=\"Pos_collection\",\n",
    "                    data=pos_data\n",
    "                )\n",
    "\n",
    "            # Insert into Img_collection\n",
    "            if data.get('extracted_images'):\n",
    "                for img_info in data['extracted_images']:\n",
    "                    img_embedding = self.extractor(img_info['path'])\n",
    "                    img_data = [{\n",
    "                        'file_name': data['file_name'],\n",
    "                        'img_file_name': img_info['filename'],\n",
    "                        'img_file_path': img_info['path'],\n",
    "                        'dense': img_embedding.tolist()\n",
    "                    }]\n",
    "                    self.client.insert(\n",
    "                        collection_name=\"Img_collection\",\n",
    "                        data=img_data\n",
    "                    )\n",
    "\n",
    "            # Flush collections to ensure data is written\n",
    "            self.client.flush(collection_name=\"Text_collection\")\n",
    "            self.client.flush(collection_name=\"Named_collection\")\n",
    "            self.client.flush(collection_name=\"Pos_collection\")\n",
    "            self.client.flush(collection_name=\"Img_collection\")\n",
    "\n",
    "      except Exception as e:\n",
    "            raise Exception(f\"Error inserting data into Milvus: {str(e)}\")\n",
    "\n",
    "\n",
    "    def milvuskeyword_search(self,keyword:str)->List[Dict[str, Any]]:\n",
    "      \"\"\"\n",
    "        Perform hybrid search using both sparse and dense vectors.\n",
    "\n",
    "        Args:\n",
    "            keyword (str): Search keyword/query\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of search results with scores and metadata\n",
    "      \"\"\"\n",
    "\n",
    "      try:\n",
    "\n",
    "        # Generate text embeddings\n",
    "        embeddings = self.bge.encode_documents([keyword])\n",
    "        sparse_vector = embeddings['sparse']\n",
    "        dense_vector = embeddings['dense'][0]\n",
    "\n",
    "        search_par_1={\n",
    "            \"data\":[dense_vector],\n",
    "            \"anns_field\":\"dense\",\n",
    "            \"param\":{\n",
    "                \"metric_type\":\"IP\",\n",
    "                \"params\":{\"nprobe\":10}\n",
    "                },\n",
    "                \"limit\":2\n",
    "                }\n",
    "        req1=AnnSearchRequest(**search_par_1)\n",
    "\n",
    "        search_par_2={\n",
    "            \"data\":[sparse_vector],\n",
    "            \"anns_field\":\"sparse\",\n",
    "            \"param\":{\n",
    "                \"metric_type\":\"IP\",\n",
    "                \"params\":{\"drop_ratio_build\":0.2}\n",
    "                },\n",
    "                \"limit\":2\n",
    "                }\n",
    "        req2=AnnSearchRequest(**search_par_2)\n",
    "\n",
    "        reqs=[req1,req2]\n",
    "        rank=RRFRanker(100)\n",
    "\n",
    "        res = self.client.hybrid_search(\n",
    "            collection_name=\"Text_collection\",\n",
    "            reqs=reqs,\n",
    "            ranker=rank,\n",
    "            limit=4\n",
    "            )\n",
    "        result=[]\n",
    "        for hits in res:\n",
    "         for hit in hits:\n",
    "          d={\n",
    "              'file_name':hit['id'],\n",
    "              'score':hit['distance'],\n",
    "              'text':self.client.get(\n",
    "                  collection_name=\"Text_collection\",\n",
    "                  ids=[hit['id']],\n",
    "                  output_fields=[\"Text\"]\n",
    "              )\n",
    "          }\n",
    "          result.append(d)\n",
    "        return result\n",
    "\n",
    "      except Exception as e:\n",
    "        raise Exception(f\"Error performing hybrid search: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def milvusimage_search(self,path:str)->List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar images in the database.\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to the query image\n",
    "            limit (int): Maximum number of results to return\n",
    "            distance_threshold (float): Maximum distance threshold for similarity\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of similar images with metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "             embeddings=self.extractor(path)\n",
    "\n",
    "             search_params = {\n",
    "                \"metric_type\": \"COSINE\",\n",
    "            }\n",
    "\n",
    "             # Execute search\n",
    "             results = self.client.search(\n",
    "                \"Img_collection\",\n",
    "                data=[embeddings],\n",
    "                anns_field=\"dense\",\n",
    "                search_params=search_params,\n",
    "                limit=2,\n",
    "                output_fields=[\"file_name\", \"img_file_name\", \"img_file_path\"]\n",
    "            )\n",
    "             # Process results\n",
    "             processed_results = []\n",
    "             for hits in results:\n",
    "                for hit in hits:\n",
    "                    result = {\n",
    "                        \"distance\":hit['distance'],\n",
    "                        \"image_file_name\": hit['entity']['img_file_name'],\n",
    "                        \"image_file_path\": hit['entity']['img_file_path'],\n",
    "                        \"document_name\": hit['entity']['file_name'],\n",
    "                        \"document_text\": self.client.get(\n",
    "                                         collection_name=\"Text_collection\",\n",
    "                                         ids=[hit['entity']['file_name']],\n",
    "                                         output_fields=[\"Text\"]\n",
    "              )\n",
    "                    }\n",
    "                    processed_results.append(result)\n",
    "\n",
    "             return processed_results\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error performing image search: {str(e)}\")\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Release collection resources.\"\"\"\n",
    "        self.text_collection.release()\n",
    "        self.named_collection.release()\n",
    "        self.pos_collection.release()\n",
    "        self.img_collection.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvdb=MilvusDatabaseHandler(\"/milvus_demo.db\")\n",
    "mvdb.store_document_data(Tresult1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import asyncio\n",
    "\n",
    "%pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    def __init__(self):\n",
    "        # Initialize LLM model and tokenizer\n",
    "        self.model_name = \"facebook/bart-large-cnn\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Define intent patterns\n",
    "        self.intent_patterns = {\n",
    "            \"DOC_SUMMARY\": r\"(?:give|show|get|what is the)?\\s*summary\\s+(?:of|for)?\\s*document\\s*(\\d+\\.jpg)\",\n",
    "            \"DOC_ENTITIES\": r\"(?:give|show|get|what are the)?\\s*(?:entities|pos|named entities|parts of speech)\\s+(?:in|of|for|present in)?\\s*document\\s*(\\d+\\.jpg)\",\n",
    "            \"ENTITY_DOCS\": r\"(?:give|show|get|what are the)?\\s*documents\\s+containing\\s+entities\\s+(?:like|such as)?\\s*(\\w+)\",\n",
    "            \"TOPIC_DOCS\": r\"(?:give|show|get|what are the)?\\s*documents\\s+containing\\s+information\\s+about\\s+(.*)\",\n",
    "            \"SIMILAR_IMAGE_DOCS\": r\"(?:given this image|given image|with this image|using this image)?\\s*(?:return|find|get|show)?\\s*(?:documents containing similar images|similar images|documents with similar images|images like this)\"\n",
    "        }\n",
    "        self.sqlconnection=PostGreDatabaseHandler(\"postgresql://postgres:12072024@localhost:5432/postgres\")\n",
    "\n",
    "    def classify_intent(self, query: str) -> Tuple[str, Optional[Dict[str, str]]]:\n",
    "        \"\"\"Classify the intent of the user query and extract parameters.\"\"\"\n",
    "        query = query.lower().strip()\n",
    "        \n",
    "        for intent, pattern in self.intent_patterns.items():\n",
    "            match = re.search(pattern, query)\n",
    "            if match:\n",
    "                params = self._extract_parameters(intent, match)\n",
    "                return intent, params\n",
    "                \n",
    "        return \"UNKNOWN\", None\n",
    "\n",
    "    def _extract_parameters(self, intent: str, match) -> Dict[str, str]:\n",
    "        \"\"\"Extract parameters based on the intent and regex match.\"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        if intent == \"DOC_SUMMARY\":\n",
    "            # Extract the complete filename (numbers.jpg)\n",
    "            params[\"doc_id\"] = match.group(1)\n",
    "            \n",
    "        elif intent == \"DOC_ENTITIES\":\n",
    "            # Extract the complete filename (numbers.jpg)\n",
    "            params[\"doc_id\"] = match.group(1)\n",
    "            params[\"entity_type\"] = \"entities\" if \"entities\" in match.group(0) else \"pos\"\n",
    "            \n",
    "        elif intent == \"ENTITY_DOCS\":\n",
    "            params[\"entity_type\"] = match.group(1).upper()\n",
    "            \n",
    "        elif intent == \"TOPIC_DOCS\":\n",
    "            params[\"topic\"] = match.group(1).strip()\n",
    "            \n",
    "        elif intent == \"SIMILAR_IMAGE_DOCS\":\n",
    "            params[\"path\"] = \"\"\n",
    "            \n",
    "        return params\n",
    "\n",
    "    async def execute_db_query(self, intent: str, params: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Placeholder for database query execution.\"\"\"\n",
    "        if intent == \"DOC_SUMMARY\":\n",
    "            result=self.sqlconnection.postgres_file_summary(params['doc_id'])\n",
    "            if not result:\n",
    "                return {'content':[]}\n",
    "            return {\"content\":[result['content']]}\n",
    "            \n",
    "        elif intent == \"DOC_ENTITIES\":\n",
    "            result=self.sqlconnection.postgres_file_entities(params['doc_id'])\n",
    "            if not result:\n",
    "               return {'entities': [], 'label': []}\n",
    "            x = {'entities':[result[i]['entity']for i in range(5)],\n",
    "               'label':[result[i]['label']for i in range(5)]}\n",
    "            return x\n",
    "              \n",
    "        elif intent == \"ENTITY_DOCS\":\n",
    "            result=self.sqlconnection.postgres_entities_file(params['entity_type'])\n",
    "            if not result:\n",
    "                return {'file name':[]}\n",
    "            return {'file name':[result[i]['file_name']for i in range(len(result))]}\n",
    "\n",
    "        elif intent == \"TOPIC_DOCS\":\n",
    "            result=mvdb.milvuskeyword_search(params['topic'])\n",
    "            if not result:\n",
    "              return {'file name':[]}\n",
    "            return {'file name':[x[i]['file_name']for i in range(len(x))]}\n",
    "\n",
    "        elif intent == \"SIMILAR_IMAGE_DOCS\":\n",
    "            result=mvdb.milvusimage_search(params['path'])\n",
    "            if not result:\n",
    "                return {'similar_docs':[]}\n",
    "            return {'document name':[result[i]['document_name'] for i in range(min(2,len(result)))]}\n",
    "\n",
    "            \n",
    "        return {\"error\": \"Unknown intent\"}\n",
    "    \n",
    "    def generate_llm_prompt(self, intent: str, db_results: Dict[str, Any], original_query: str) -> str:\n",
    "        \"\"\"Generate appropriate prompt for LLM based on intent and results.\"\"\"\n",
    "        match intent:\n",
    "\n",
    "            case \"DOC_SUMMARY\":\n",
    "                return f\"\"\"\n",
    "                You are a skilled document analyst tasked with creating a comprehensive yet concise summary. Please analyze the following document content:\n",
    "                {db_results['content']}\n",
    "                Provide a summary that:\n",
    "                1. Captures the main ideas and key points\n",
    "                2. Maintains the original meaning and intent\n",
    "                3. Preserves important details and supporting evidence\n",
    "                4. Uses clear, professional language\n",
    "                5. Follows a logical flow\n",
    "\n",
    "                Format the summary in well-organized paragraphs. Include:\n",
    "                - A brief overview of the document's main topic/purpose\n",
    "                - The key arguments or findings\n",
    "                - Important supporting details or examples\n",
    "                - Any significant conclusions or implications\n",
    "\n",
    "                If the document contains technical terms, numbers, or data, incorporate them accurately.\n",
    "                Aim for a length that balances completeness with conciseness - typically 15-25% of the original length.\n",
    "                If you're unsure about any part of the content, maintain factual accuracy by focusing on the clearly stated information.\n",
    "                \"\"\"\n",
    "            \n",
    "            case \"DOC_ENTITIES\":\n",
    "\n",
    "                entity_label_pairs = [\n",
    "                    f\"- Entity: {entity}\\n  Label: {label}\"\n",
    "                    for entity, label in zip(db_results['entities'], db_results['label'])\n",
    "                    ]\n",
    "                formatted_entities = \"\\n\".join(entity_label_pairs)\n",
    "                return f\"\"\"\n",
    "                You are a skilled entity analyzer tasked with explaining the named entities found in the text. Here are the entities and their labels:\n",
    "                {formatted_entities}\n",
    "                \n",
    "                Please provide:\n",
    "                1. A clear explanation of each entity and its classification\n",
    "                2. Any patterns or relationships between the entities\n",
    "                3. Context about why these entities might be important\n",
    "                4. Group similar entities together (e.g., all organizations, all works of art)\n",
    "\n",
    "                Format your response as follows:\n",
    "                - Start with a brief overview of the types of entities found\n",
    "                - Group entities by their labels\n",
    "                - For each entity, explain its significance and why it was classified as its given label\n",
    "                - Note any interesting patterns or relationships between entities\n",
    "\n",
    "                If there are multiple mentions of the same entity with slight variations (e.g., \"ABC\" and \"ABC Family\"), explain the relationship between these variations.\n",
    "\n",
    "                Keep your response clear and professional, focusing on accuracy and meaningful insights about the entities and their roles.\n",
    "                \"\"\"\n",
    "            \n",
    "            case \"ENTITY_DOCS\":\n",
    "                # Format the list of filenames\n",
    "                files = \"\\n\".join(f\"- {filename}\" for filename in db_results['file name'])\n",
    "                return f\"\"\"\n",
    "                You are a document retrieval specialist tasked with explaining the search results for entity-related documents. The following files contain the requested entity:\n",
    "                {files}\n",
    "\n",
    "                Please provide:\n",
    "                1. A clear overview of the search results\n",
    "                2. The number of documents found\n",
    "\n",
    "                Format your response as follows:\n",
    "                - Start with a summary of the search results (e.g., \"Found X documents containing the requested entity\")\n",
    "                - List the documents in a clear, organized manner\n",
    "\n",
    "                Keep your response:\n",
    "                - Clear and professional\n",
    "                - Focused on helping the user understand what documents are available\n",
    "                - Organized in a way that makes the search results easy to understand\n",
    "                \"\"\"\n",
    "            case \"TOPIC_DOCS\":\n",
    "                # Format the list of document names\n",
    "                topic_docs = \"\\n\".join(f\"- {doc}\" for doc in db_results['documents'])\n",
    "                return f\"\"\"\n",
    "                You are a topic search specialist tasked with explaining document search results. The following analysis is based on:\n",
    "                Original Query: {original_query}\n",
    "\n",
    "                Documents found containing relevant information:\n",
    "                {topic_docs}\n",
    "\n",
    "                Please provide:\n",
    "                1. A clear explanation of how these documents relate to the query topic\n",
    "                2. The total number of relevant documents found\n",
    "                3. Any patterns in the type of information available\n",
    "                4. Recommendations for which documents might be most relevant to the query\n",
    "\n",
    "                Format your response as follows:\n",
    "                - Start with a restatement of the search query and what was found\n",
    "                - List the relevant documents in order of likely importance\n",
    "                - Explain why each document might contain relevant information\n",
    "                - Suggest a reading order that would best address the original query\n",
    "\n",
    "                Important considerations:\n",
    "                - Focus on the connection between the documents and the query topic\n",
    "                - Note if there appear to be different aspects of the topic covered\n",
    "                - Highlight documents that seem most directly relevant to the query\n",
    "                - Consider how the documents might complement each other in answering the query\n",
    "\n",
    "                Keep your response:\n",
    "                - Focused on helping the user understand why these documents were found\n",
    "                - Clear about the relationship between the documents and the query\n",
    "                - Organized to help the user efficiently find the information they need\n",
    "                If you notice any patterns in how the topic is covered across documents, include these insights to help guide the user's reading.\n",
    "                \"\"\"\n",
    "                \n",
    "            case \"SIMILAR_IMAGE_DOCS\":\n",
    "                # Format the list of filenames\n",
    "                similar_files = \"\\n\".join(f\"- {filename}\" for filename in db_results['file name'])\n",
    "                return f\"\"\"\n",
    "                You are an image search specialist tasked with explaining the results of a visual similarity search. The following images were found to be visually similar to the query image:\n",
    "                {similar_files}\n",
    "\n",
    "                Please provide:\n",
    "                1. A clear overview of the search results\n",
    "                2. The total number of similar images found\n",
    "                3. Any patterns in the types of images discovered\n",
    "                4. Recommendations for which images might be most relevant\n",
    "\n",
    "                Format your response as follows:\n",
    "                - Begin with a summary of the visual search results (e.g., \"Found X visually similar images\")\n",
    "                - List the similar images in a clear, organized manner\n",
    "                - If there are patterns in the image names or types, explain their significance\n",
    "                - Provide context about why these images might be similar to the query image\n",
    "\n",
    "                Important considerations:\n",
    "                - Focus on explaining the potential visual similarities\n",
    "                - Note if there appear to be groups or clusters of similar images\n",
    "                - Highlight any particularly strong matches based on the file organization\n",
    "                - Suggest which images might be most worth reviewing first\n",
    "\n",
    "                Keep your response clear and professional, helping the user understand the visual similarity search results and their potential relevance.\n",
    "                \"\"\"\n",
    "            \n",
    "    async def generate_llm_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response using the LLM.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=200,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    async def process_query(self, query: str, image_data: Optional[bytes] = None) -> str:\n",
    "        \"\"\"Main method to process user query and generate response.\"\"\"\n",
    "        # 1. Classify intent and extract parameters\n",
    "        intent, params = self.classify_intent(query)\n",
    "        \n",
    "        if intent == \"UNKNOWN\":\n",
    "            return \"I'm sorry, I couldn't understand your query. Please rephrase it.\"\n",
    "            \n",
    "        # 2. Check if image is required but not provided\n",
    "        if intent == \"SIMILAR_IMAGE_DOCS\" and image_data is None:\n",
    "            return \"Please provide an image for image similarity search.\"\n",
    "        elif intent==\"DOC_SUMMARY\":\n",
    "            return await self.execute_db_query(intent, params)\n",
    "            \n",
    "        # 3. Execute database query\n",
    "        db_results = await self.execute_db_query(intent, params)\n",
    "        print(\"db result\",db_results)\n",
    "        # 4. Generate LLM prompt\n",
    "        prompt = self.generate_llm_prompt(intent, db_results, query)\n",
    "        print(\"prompt\",prompt)\n",
    "        # 5. Generate final response\n",
    "        response = await self.generate_llm_response(prompt)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "processor = QueryProcessor()\n",
    "\n",
    "# Function to test queries\n",
    "async def test_query(query: str, image_data=None):\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Get intent and parameters\n",
    "    intent, params = processor.classify_intent(query)\n",
    "    print(f\"Detected Intent: {intent}\")\n",
    "    print(f\"Extracted Parameters: {params}\")\n",
    "    \n",
    "    # Get response\n",
    "    response = await processor.process_query(query, image_data)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Create event loop\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple queries\n",
    "test_queries = [\n",
    "    \"give summary of document X123\",\n",
    "    \"show entities present in document Y456\",\n",
    "    \"give documents containing entities\",\n",
    "    \"give documents containing information about climate change\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    loop.run_until_complete(test_query(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
